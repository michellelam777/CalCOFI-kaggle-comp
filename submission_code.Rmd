---
title: "CalCOFI Kaggle Competition"
author: "Michelle Lam & Alexandria Reed"
date: "2023-03-16"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(skimr)
library(rpart) # direct engine for decision tree application
library(caret) # meta engine for decision tree application
library(janitor)
library(ranger)
library(xgboost)
library(gridExtra)
library(here)
library(tictoc)
```

# Read in the Data
```{r}
training_data <- read_csv(here("data","train.csv")) |> 
  clean_names() |> 
  dplyr::select(-c(x13, id))
```

# Explore the Data

## Plot the data to see if the relationships between features and DIC are linear
```{r}
lat_plot <- ggplot(data = training_data, aes(x = lat_dec, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

lon_plot <- ggplot(data = training_data, aes(x = lon_dec, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

nitrite_plot <- ggplot(data = training_data, aes(x = no2u_m, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

# linear
nitrate_plot <- ggplot(data = training_data, aes(x = no3u_m, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

ammonia_plot <- ggplot(data = training_data, aes(x = nh3u_m, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

reported_temp_plot<- ggplot(data = training_data, aes(x = r_temp, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

reported_depth_plot <- ggplot(data = training_data, aes(x = r_depth, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

reported_salinity_plot <- ggplot(data = training_data, aes(x = r_sal, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

dynamic_height_plot <- ggplot(data = training_data, aes(x = r_dynht, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

ammonium_plot <- ggplot(data = training_data, aes(x = r_nuts, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

# linear
oxygen_plot <- ggplot(data = training_data, aes(x = r_oxy_micromol_kg, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

# linear
phosphate_plot <- ggplot(data = training_data, aes(x = po4u_m, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

silicate_plot <- ggplot(data = training_data, aes(x = si_o3u_m, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

alkalinity_plot <- ggplot(data = training_data, aes(x = ta1_x, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

salinity_plot <- ggplot(data = training_data, aes(x = salinity1, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

temp_plot <- ggplot(data = training_data, aes(x = temperature_deg_c, y = dic)) + 
  geom_point(color = "darkorchid") +
  theme_minimal()

# arrange side by side
grid.arrange(lat_plot, lon_plot, nitrite_plot, nitrate_plot, ammonia_plot, 
             reported_temp_plot, reported_depth_plot, reported_salinity_plot, 
             dynamic_height_plot, ammonium_plot, oxygen_plot, phosphate_plot, 
             silicate_plot, alkalinity_plot, salinity_plot, temp_plot, ncol = 4, nrow = 4)
```

**Based on the scatterplots above, we noticed that most of the relationships between the predictors and outcome are non-linear; therefore, we decided to move forward with a XGBoost model. Additionally, there were some observations that contained outliers and XGBoost models are robust to outliers.**

## Create a correlation matrix to see which features are most highly correlated with DIC
```{r}
# Obtain correlation matrix for variables
corr_mat <- cor(training_data)

library(corrplot)

corrplot(corr_mat, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", number.cex = 0.5, cl.pos = "n", order = "original")
```

##Preprocessing
```{r}
#initial split of data
set.seed(160)
dic_split <- initial_split(training_data, prop = 0.7)
dic_train <- training(dic_split)
dic_test <- testing(dic_split)

#Create a new recipe
recipe <- recipe(dic ~., data = dic_train) |>
  step_center(all_numeric(), -all_outcomes()) |>
  step_scale(all_numeric(), -all_outcomes()) |> 
  prep(dic_train) 

#cv folds
set.seed(160)
cv_folds <- dic_train |>
  vfold_cv(v = 10)
```

## See baseline model performance - no hyperparameter tuning
```{r}
# baseline model performance - defualt hyperparameter values
# specify the baseline model
xgboost_base_model <- 
  parsnip::boost_tree(
    mode = "regression",
  ) |> 
    set_engine("xgboost")

# define a workflow
xgboost_base_wf <- 
  workflow() |> 
  add_model(xgboost_base_model) |> 
  add_recipe(recipe)

# fit the model and test RMSE
xgboost_base_fit <- last_fit(xgboost_base_wf, dic_split)

xgboost_base_fit |> collect_metrics()

```

## Specify the first model
```{r}
# specify the first model
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    learn_rate = tune(),
  ) |> 
    set_engine("xgboost")
```

## Tune Learn Rate
```{r}
# set up tuning grid using a range of learning rate values
learn_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

# define a workflow
xgboost_wf <- 
  workflow() |> 
  add_model(xgboost_model) |> 
  add_recipe(recipe)

# hyperparameter tuning
tic()
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = cv_folds,
  grid = learn_grid,
  metrics = yardstick::metric_set(rmse))
toc()

# show best learn rate
show_best(xgboost_tuned)

# store the best learning rate value
best_learn_rate <- select_best(xgboost_tuned)
best_learn_rate_value <- best_learn_rate$learn_rate
```

## Tune Tree Parameters
```{r}
# create a new model spec to tune tree parameters with learning rate set
xgboost_model2 <- 
  parsnip::boost_tree(
    mode = "regression",
    learn_rate = best_learn_rate_value,
    trees = tune(),
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune()
  ) |> 
    set_engine("xgboost")

# set up a tuning grid for tree_depth, min_n, and loss_reduction
tree_params <- parameters(tree_depth(), min_n(), loss_reduction(), trees())

tree_grid<- grid_max_entropy(tree_params, 
                             size = 30,
                             iter = 100)

# define a new workflow with the new model spec
xgboost_wf2 <- 
  workflow() |> 
  add_model(xgboost_model2) |> 
  add_recipe(recipe)

# hyperparameter tuning
tic()
xgboost_tuned2 <- tune::tune_grid(
  object = xgboost_wf2,
  resamples = cv_folds,
  grid = tree_grid,
  metrics = yardstick::metric_set(rmse))
toc()

# show best tree parameters
show_best(xgboost_tuned2)

# store the best tree_param, tree_depth, and loss_reduction values
best_tree_params <- select_best(xgboost_tuned2)
best_min_n <- best_tree_params$min_n
best_tree_depth <- best_tree_params$tree_depth
best_loss_reduction <- best_tree_params$loss_reduction
best_tree_n <- best_tree_params$trees
```

## Tune Stochastic Parameters
```{r}
# create a new  model spec to tune stochastic params (learn rate and tree params set)
xgboost_model3 <- 
  parsnip::boost_tree(
    mode = "regression",
    learn_rate = best_learn_rate_value,
    trees = best_tree_n,
    tree_depth = best_tree_depth,
    min_n = best_min_n,
    loss_reduction = best_loss_reduction,
    mtry = tune(),
    sample_size = tune()
  ) |> 
    set_engine("xgboost")

# set up a tuning grid for mtry and sample_prop 
stochastic_grid <- grid_max_entropy(finalize(mtry(), dic_train |> 
                                    dplyr::select(-dic)), sample_prop(), 
                                    size = 30,
                                    iter = 100)

# define a new workflow with new model spec
xgboost_wf3 <- 
  workflow() |> 
  add_model(xgboost_model3) |> 
  add_recipe(recipe)

# hyperparameter tuning
tic()
xgboost_tuned3 <- tune::tune_grid(
  object = xgboost_wf3,
  resamples = cv_folds,
  grid = stochastic_grid,
  metrics = yardstick::metric_set(rmse))
toc()

# show best performance models and corresponding stochastic parameters
show_best(xgboost_tuned3)

# store the best mtry and sample_prop values
best_stochastic_params <- select_best(xgboost_tuned3)
best_mtry <- best_stochastic_params$mtry
best_sample_size <- best_stochastic_params$sample_size
```

## Retune learn rate
```{r}
# create a new model spec to retune learn rate
xgboost_model4 <- 
  parsnip::boost_tree(
    mode = "regression",
    learn_rate = tune(),
    trees = best_tree_n,
    tree_depth = best_tree_depth,
    min_n = best_min_n,
    loss_reduction = best_loss_reduction,
    mtry = best_mtry,
    sample_size = best_sample_size
  ) |> 
    set_engine("xgboost")

# define a new workflow with new model spec
xgboost_wf4 <- 
  workflow() |> 
  add_model(xgboost_model4) |> 
  add_recipe(recipe)

# hyperparameter tuning
tic()
xgboost_tuned4 <- tune::tune_grid(
  object = xgboost_wf4,
  resamples = cv_folds,
  grid = learn_grid,
  metrics = yardstick::metric_set(rmse))
toc()

# show best performance models and corresponding stochastic parameters
show_best(xgboost_tuned4)

# store the best mtry and sample_prop values
best_learn_rate_v2 <- select_best(xgboost_tuned4)
```

## Finalize the Workflow and View Model Performance
```{r}
# create full dataframe with hyperparameters
all_hyperparam <- cbind(best_learn_rate_v2, best_tree_params, best_stochastic_params) |>
  dplyr::select(-.config)

# finalize the workflow
final_xgboost_wf <- finalize_workflow(xgboost_wf4, parameters = all_hyperparam)

# fit the final model on training dataset & make predictions on testing dataset
final_xgboost_fit <- last_fit(final_xgboost_wf, dic_split)

# see how well model performed
final_xgboost_fit |>  collect_metrics()
```

## Make predictions on test data
```{r}
# fit final workflow to training data
final_xgboost_fit_2 <- final_xgboost_wf |> 
  fit(data = training_data)

# load in test datset and clean
test_data <- read_csv(here("data","test.csv")) |> 
  clean_names() |> 
  rename(ta1_x = ta1)

# make prediction on new test data
final_xgboost_pred <- final_xgboost_fit_2 |> 
  predict(new_data = test_data)

# combine predictions with id
submission_csv_xgboost <- cbind(test_data, final_xgboost_pred) |> 
  dplyr::select(c(id, .pred)) |> 
  rename(DIC = .pred)

# export csv
#write.csv(submission_csv_xgboost, here("michelle_alex_submission_xgboost.csv"), row.names = FALSE)
```

